{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activaton Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sigmoid functions ----> forward and backward\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    #print(\"\\nShape of A after sigmoidizing-->\", A.shape)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### Relu functions ----> forward and backward\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    np.random.seed(31)\n",
    "    \n",
    "    for i in range(1, len(layer_dims)):\n",
    "        \n",
    "        parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "        assert( parameters['W' + str(i)].shape == (layer_dims[i], layer_dims[i-1]) )\n",
    "        assert( parameters['b' + str(i)].shape == (layer_dims[i], 1) )\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = init_parameters([5, 3, 3, 2])\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_model(train_data, parameters):\n",
    "    \n",
    "    A = train_data\n",
    "    L = len(parameters) // 2\n",
    "    caches = {}\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters['W'+ str(i)]\n",
    "        b = parameters['b'+ str(i)]\n",
    "        \n",
    "        Z = np.dot(W , A_prev) + b\n",
    "        assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        caches['linear' + str(i)] = (A_prev, W, b)\n",
    "        \n",
    "        A, caches['activation' + str(i)] = relu(Z)\n",
    "        assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        \n",
    "    \n",
    "    Z = np.dot(parameters['W'+str(L)], A) + parameters['b' + str(L)]    \n",
    "    assert(Z.shape == (parameters['W'+str(L)].shape[0], A.shape[1]))\n",
    "    \n",
    "    \n",
    "    AL, caches['activation' + str(L)] = sigmoid(Z)\n",
    "    assert(AL.shape == (parameters['W'+str(L)].shape[0], A.shape[1]))\n",
    "    \n",
    "    caches['linear' + str(L)] = (A, parameters['W'+str(L)], parameters['b'+str(L)])\n",
    "    \n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = feed_forward_model(np.random.randn(5,7), init_param([5, 3, 3, 2]))\n",
    "print(\"\\n\" , AL.shape)\n",
    "#cache_for_back_prop = caches\n",
    "#len(cache_for_back_prop)\n",
    "\n",
    "#print(caches['activation1'].shape)\n",
    "#print(caches['activation2'].shape)\n",
    "#print(caches['activation3'].shape, type(caches['activation3']))\n",
    "#print(AL.shape, type(AL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, train_class):\n",
    "    \n",
    "    label = train_class\n",
    "    # No of training samples\n",
    "    m = label.shape[1]\n",
    "    #label = label.reshape(AL.shape)\n",
    "    \n",
    "    cost = (AL - label)**2\n",
    "    cost = (1/m) * np.cumsum(cost)[-1]\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = feed_forward_model(np.random.randn(5,7), init_param([5, 3, 3, 2]))\n",
    "Y = np.ones(14).reshape(1,14)\n",
    "\n",
    "#print(AL.shape)\n",
    "#print(Y.shape)\n",
    "\n",
    "cost = cost_function(AL, Y)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, train_class, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    Y = train_class\n",
    "    #A_prev = AL\n",
    "    L = len(caches) // 2\n",
    "    m = AL.shape[1]\n",
    "    #Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    ######## Block Starts Here ##########\n",
    "    \n",
    "    dAL = 2 * np.subtract(AL, Y)\n",
    "    \n",
    "    ######## END Here ############\n",
    "    \n",
    "    \n",
    "    ###### Block Starts Here #########\n",
    "    \n",
    "    A_prev, W , b = caches['linear' + str(L)]\n",
    "    \n",
    "    dZ = relu_backward(dAL, caches['activation' + str(L)])\n",
    "    \n",
    "    dW = (1/m) * np.dot( dZ, np.transpose(A_prev) )\n",
    "    db = (1/m) * np.sum( dZ, axis=1, keepdims=True )\n",
    "    dA_prev = np.dot( np.transpose(dW), dZ )\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    ######## END Here ##########\n",
    "    \n",
    "    grads['dA' + str(L)] = dA_prev\n",
    "    grads['dW' + str(L)] = dW\n",
    "    grads['db' + str(L)] = db\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(L-1)):\n",
    "          \n",
    "        ###### Block Starts Here ######### \n",
    "        A_prev, W , b = caches['linear' + str(i+1)]\n",
    "        \n",
    "        dZ = relu_backward(dA_prev, caches['activation' + str(i+1)])\n",
    "\n",
    "        dW_temp = (1/m) * np.dot( dZ, np.transpose(A_prev) )\n",
    "        db_temp = (1/m) * np.sum( dZ, axis=1, keepdims=True )\n",
    "        dA_prev_temp = np.dot( np.transpose(W), dZ )\n",
    "\n",
    "        assert (dA_prev_temp.shape == A_prev.shape)\n",
    "        assert (dW_temp.shape == W.shape)\n",
    "        assert (db_temp.shape == b.shape)\n",
    "        ######## END Here ##########\n",
    "        \n",
    "        dW_temp = (1/m) * np.dot( dZ, np.transpose(A_prev) )\n",
    "        db_temp = (1/m) * np.sum( dZ, axis=1, keepdims=True )\n",
    "        dA_prev_temp = np.dot( np.transpose(W), dZ )\n",
    "        \n",
    "        dA_prev = dA_prev_temp\n",
    "        \n",
    "        grads['dA' + str(i+1)] = dA_prev_temp\n",
    "        grads['dW' + str(i+1)] = dW_temp\n",
    "        grads['db' + str(i+1)] = db_temp\n",
    "        \n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = feed_forward_model(np.random.randn(5,7), init_param([5, 3, 3, 2]))\n",
    "Y = np.ones(14).reshape(1,14)\n",
    "caches = cache_for_back_prop\n",
    "\n",
    "grad_desc = backward_propagation(AL, Y, caches)\n",
    "\n",
    "#grad_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradient, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(L):\n",
    "        parameters['W' + str(i+1)] = parameters['W' + str(i+1)] - learning_rate * gradient['dW' + str(i+1)]\n",
    "        parameters['b' + str(i+1)] = parameters['b' + str(i+1)] - learning_rate * gradient['db' + str(i+1)]\n",
    "        \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(data, label, parameters):\n",
    "    \n",
    "    m = data.shape[1]\n",
    "    prediction = np.zeros((1, m))\n",
    "    #label = label.reshape()\n",
    "    \n",
    "    output, caches = feed_forward_model(data, parameters)\n",
    "    \n",
    "    for i in range(0, output.shape[1]):\n",
    "        \n",
    "        if output[0, i] >= 0.5:\n",
    "            prediction[0, i] = 1\n",
    "        else:\n",
    "            prediction[0, i] = 0\n",
    "              \n",
    "    print(f'Accuracy : {(np.sum((prediction == label) / m)) * 100}%')\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.array([[0.17,-0.21,-0.63,0.54],\n",
    "               [0.44,-0.55,0.16,0.33],\n",
    "               [0.9,0.84,0.2,0.11]])\n",
    "param = init_param([3,3,2])\n",
    "label = np.random.randint(0,2,4).reshape(1,4)\n",
    "label.shape\n",
    "prediction = prediction_function(data, label, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Mislabeled Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mislabeled_images(classes, data, label, prediction):\n",
    "    X = data\n",
    "    y = label\n",
    "    p = prediction\n",
    "\n",
    "    mislabeled_indices = np.asarray(np.where((p+y) == 1))\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the one call NN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deep_Net(train_data, train_class, layer_dims, learning_rate, num_iterations):\n",
    "    \n",
    "    # initializing paramters\n",
    "    parameters = init_parameters(layer_dims)\n",
    "    \n",
    "    print_cost = False\n",
    "    costs = []\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i in range(0, num_iterations+1):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        #training the model\n",
    "        AL, caches = feed_forward_model(train_data, parameters)\n",
    "\n",
    "        #computing cost\n",
    "        cost = cost_function(AL, train_class)\n",
    "\n",
    "        #back propagation\n",
    "        gradients = backward_propagation(AL, train_class, caches)\n",
    "\n",
    "        #updating the parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        if ( 1 and i % 100 == 0):\n",
    "            print(f'Cost after {i}th iterations = {cost}')\n",
    "            #print(f'Time taken for {i} iterations = {time.time()-t1}Sec.\\n')\n",
    "            costs.append(cost)\n",
    "    \n",
    "    print(f'Total time taken = {(time.time()-t0)}Sec.')\n",
    "    \n",
    "    print(f'\\nTraining Accuracy-->')\n",
    "    prediction = prediction_function(train_data, train_class, parameters)\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return parameters, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_data = np.array([[0.17,-0.21,-0.63,0.54,0.17,-0.21],\n",
    "               [0.44,-0.55,0.16,0.33,0.54,0.17],\n",
    "               [0.9,0.84,0.2,0.11,0.54,0.17],\n",
    "               [0.44,-0.55,0.16,0.33,0.54,0.17],\n",
    "               [0.17,-0.21,-0.63,0.54,0.54,0.17]])\n",
    "'''\n",
    "np.random.seed(10)\n",
    "\n",
    "train_data1 = np.random.randn(784,50)\n",
    "train_class1 = np.random.randint(0,6,50).reshape(1,50)\n",
    "train_class1.shape\n",
    "#param, pred = Deep_Net(train_data, train_class, [784,16,16,12,3], 0.00075, 300)\n",
    "\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test prediction-->')\n",
    "test_pred = prediction_function(np.random.randn(784,20), np.random.randint(0,6,20).reshape(1,20), param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load train_data_set\n",
    "\n",
    "with h5py.File(r'C:\\Users\\Asif\\Desktop\\DeepLearning_Codes_Python\\Deep-Learning-Coursera-master2222\\My_notebooks/train_catvnoncat.h5', 'r') as train_dset:\n",
    "    ls = list(train_dset.keys())\n",
    "    \n",
    "    classes = train_dset.get('list_classes')\n",
    "    classes = np.array(classes)\n",
    "    \n",
    "    data = train_dset.get('train_set_x')\n",
    "    train_x = np.array(data)\n",
    "    \n",
    "    label = train_dset.get('train_set_y')\n",
    "    train_y = np.array(label)\n",
    "    \n",
    "\n",
    "#classes[0]\n",
    "#train_x.shape\n",
    "#train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load test_data_set\n",
    "\n",
    "with h5py.File(r'C:\\Users\\Asif\\Desktop\\DeepLearning_Codes_Python\\Deep-Learning-Coursera-master2222\\My_notebooks/test_catvnoncat.h5', 'r') as test_dset:\n",
    "    ls = list(test_dset.keys())\n",
    "    \n",
    "    data = test_dset.get('test_set_x')\n",
    "    test_x = np.array(data)\n",
    "    \n",
    "    label = test_dset.get('test_set_y')\n",
    "    test_y = np.array(label)\n",
    "\n",
    "\n",
    "#test_x.shape\n",
    "#test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_image = 208\n",
    "\n",
    "plt.imshow(train_x[idx_image])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping and standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train set Before... Data-->(209, 64, 64, 3), label-->(209,)\n",
      "Shape of Test set Before... Data-->(50, 64, 64, 3), label-->(50,)\n",
      "\n",
      "Shape of Train set After... Data-->(12288, 209), label-->(1, 209)\n",
      "Shape of Test set After... Data-->(12288, 50), label-->(1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Falttening the images\n",
    "\n",
    "train_x_faltten = train_x.reshape(train_x.shape[0], -1).T\n",
    "test_x_faltten = test_x.reshape(test_x.shape[0], -1).T\n",
    "\n",
    "train_label = train_y.reshape(1, train_y.shape[0])\n",
    "test_label = test_y.reshape(1, test_y.shape[0])\n",
    "\n",
    "# Standardizing the flatten image\n",
    "train_data = train_x_faltten / 255\n",
    "test_data = test_x_faltten / 255\n",
    "\n",
    "print(f'Shape of Train set Before... Data-->{train_x.shape}, label-->{train_y.shape}')\n",
    "print(f'Shape of Test set Before... Data-->{test_x.shape}, label-->{test_y.shape}')\n",
    "\n",
    "print(f'\\nShape of Train set After... Data-->{train_data.shape}, label-->{train_label.shape}')\n",
    "print(f'Shape of Test set After... Data-->{test_data.shape}, label-->{test_label.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Deep Neural Net with the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0th iterations = 0.248231825804116\n",
      "Cost after 100th iterations = 0.248231825804116\n",
      "Cost after 200th iterations = 0.248231825804116\n",
      "Total time taken = 10.744340419769287Sec.\n",
      "\n",
      "Training Accuracy-->\n",
      "Accuracy : 65.55023923444976%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVcAAAEWCAYAAADSNdTRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAdS0lEQVR4nO3de7xcVX338c+XBIIQuQQCRnIBbFBCoaAjikqhCDVQCGhRgYCgtDRQ7FOpPkSpSvHhKRJ5UAvWBOT2CHL1EhBEQC6tGMyJJIEEIyGKCRFzuCcGE4O//rHWwGacSeacMyuH5Hzfr9e8zuy1195rrTPJ9+zZe2ZtRQRmZtZZm/R3B8zMNkYOVzOzAhyuZmYFOFzNzApwuJqZFeBwNTMrwOFqr1mSbpN0Yn/3w6w3HK72JyT9StLB/d2PiDg0Iq7s734ASLpH0t+th3Y+JOl+SSsl3dOB/R0n6XFJv5P0XUnDKuvukfR7SSvyY0Ff27NXOFytX0ga3N99qHst9QV4BvgycF5fdyRpD2AqcAKwI7AS+FpDtdMjYmh+vLmvbdorHK7WI5IOlzRb0nP5CGuvyrrJkh6TtFzSfEnvr6w7SdKPJV0o6Rng7Fz235K+JOlZSb+UdGhlm5ePFtuou4uk+3Lbd0q6WNI3W4zhQElLJJ0p6UngcknbSrpFUnfe/y2SRub65wL7AxflI7yLcvlbJN0h6RlJCyR9qK+/34i4MyKuB5a26Ps78+/9OUlzJB24lt1NBG6OiPsiYgXwWeADkl7f137aujlcrW2S3gpcBvwDsB3pqGi6pCG5ymOkENoa+Dfgm5JGVHbxDmARsANwbqVsAbA9cD7wDUlq0YW11b0G+Gnu19mko7W1eQMwDBgDnEL6v3B5Xh4NvAhcBBARZwH/xStHeadL2hK4I7e7A3As8LV8tPgnJH0tB2Kzx9x19LW+j52A7wP/J/f9k8BNkoa32GQPYE59ISIeA1YDu1Xq/Lukp/IfvgPb6Ye1x+FqPfH3wNSIeCAiXsrnQ1cB7wSIiBsiYmlE/DEirgMeBfatbL80Iv4jItZExIu57PGIuCQiXgKuBEaQ3sI207SupNHA24HPRcTqiPhvYPo6xvJH4PMRsSoiXoyIpyPipohYGRHLSeF/wFq2Pxz4VURcnsfzM+Am4OhmlSPitIjYpsVjr2bbNHE8cGtE3Jp/x3cAXcBhLeoPBZ5vKHseqB+5ngnsCuwETANulvSmNvti6+BwtZ4YA/xL9agLGAW8EUDSRyqnDJ4D/px0lFm3uMk+n6w/iYiV+enQFu23qvtG4JlKWau2qroj4vf1BUlbSJqaL/68ANwHbCNpUIvtxwDvaPhdTCQdEZcyBvhgQ5vvAUZI2r9yYWperr8C2KphH1sBywHyH8nl+Q/MlcCPaR3U1kOvpRP59tq3GDg3Is5tXCFpDHAJ8F7gJxHxkqTZQPUtfqkp2H4DDJO0RSVgR61jm8a+/AvwZuAdEfGkpL2BB3ml/431FwP3RsQh7XRQ0tdJR57NPB4RTU8nNGnz/0fE37dY3/hHaR7wF5U+7AoMAX7RYvvg1a+X9YGPXK2VTSVtXnkMJoXnJEnvULKlpL/JF0i2JP3n7AaQ9FHSkWtxEfE46e3x2ZI2k7QfcEQPd/N60nnW55Q+rvT5hvW/Jb2FrrsF2E3SCZI2zY+3S9q9RR8nVa7KNz5eDlZJgyRtTjrw2ST/7jfNq78JHCHpffV6+eLcyBZjujrX3z+fIz4H+HZELJe0Td7P5pIGS5oI/CVwe49+a9aSw9VauZUUNvXH2RHRRTrvehHwLLAQOAkgIuYDFwA/IQXRnqS3mevLRGA/4GnSBZ/rSOeD2/Vl4HXAU8AM4AcN678CHJ0/SfDVfF72r4FjSFf2nwS+SDoy7IsTSL/v/yRdHHyR9EeNiFgMHAl8hvRHbDHwKVr8P46IecAkUsguI/0BOS2v3pT0e+rOY/44cFRE+LOuHSJPlm0bI0nXAT+PiMYjULP1wkeutlHIb8nfJGkTSeNJR3jf7e9+2cDlC1q2sXgD8G3S51yXAKdGxIP92yUbyHxawMysAJ8WMDMrYECcFth+++1j55137u9umNlGZtasWU9FRNOvHw+IcN15553p6urq726Y2UZG0uOt1vm0gJlZAQ5XM7MCHK5mZgU4XM3MCnC4mpkV4HA1MyvA4WpmVoDD1cysAIermVkBDlczswKKhquk8fl+7gslTW6y/gyl+9vPlXRXvg9Tfd1oST+U9Eius3Mu30XSA5IelXSdpM1KjsHMrDeKhWu+a+bFwKHAOOBYSeMaqj0I1PKthW8k3Yu+7ipgSkTsTro987Jc/kXgwogYS7rVyMmlxmBm1lslj1z3BRZGxKKIWA1cS5od/mURcXflbp0zgJEAOYQH5/uyExErImKlJAEHkYIY0r3rjyo4BjOzXikZrjvx6nvHL8llrZwM3Jaf70a6C+e3JT0oaUo+Et4OeC4i1qxrn5JOkdQlqau7u7tPAzEz66mS4drs/udNb3sg6XigBkzJRYNJd778JPB20i2NT+rJPiNiWkTUIqI2fHjT6RbNzIopGa5LgFGV5ZGkWxC/iqSDgbOACRGxqrLtg/mUwhrSjebeSroF8DaSBq9tn2Zm/a1kuM4Exuar+5uR7u8+vVpB0j7AVFKwLmvYdltJ9UPOg4D5kW74dTdwdC4/EfhewTGYmfVKsXDNR5ynA7cDjwDXR8Q8SedImpCrTQGGAjdImi1pet72JdIpgbskPUQ6HXBJ3uZM4AxJC0nnYL9RagxmZr01IO7+WqvVwrd5MbNOkzQrImrN1vkbWmZmBThczcwKcLiamRXgcDUzK8DhamZWgMPVzKwAh6uZWQEOVzOzAhyuZmYFOFzNzApwuJqZFeBwNTMrwOFqZlaAw9XMrACHq5lZAQ5XM7MCHK5mZgU4XM3MCnC4mpkV4HA1MyvA4WpmVoDD1cysAIermVkBDlczswIcrmZmBThczcwKcLiamRXgcDUzK8DhamZWgMPVzKwAh6uZWQFFw1XSeEkLJC2UNLnJ+jMkzZc0V9JdksZU1r0kaXZ+TK+UXyHpl5V1e5ccg5lZbwwutWNJg4CLgUOAJcBMSdMjYn6l2oNALSJWSjoVOB/4cF73YkS0Cs5PRcSNpfpuZtZXJY9c9wUWRsSiiFgNXAscWa0QEXdHxMq8OAMYWbA/ZmbrTclw3QlYXFlekstaORm4rbK8uaQuSTMkHdVQ99x8KuFCSUOa7UzSKXn7ru7u7l4NwMyst0qGq5qURdOK0vFADZhSKR4dETXgOODLkt6Uyz8NvAV4OzAMOLPZPiNiWkTUIqI2fPjwXg7BzKx3SobrEmBUZXkksLSxkqSDgbOACRGxql4eEUvzz0XAPcA+efk3kawCLiedfjAze00pGa4zgbGSdpG0GXAMML1aQdI+wFRSsC6rlG9bf7svaXvg3cD8vDwi/xRwFPBwwTGYmfVKsU8LRMQaSacDtwODgMsiYp6kc4CuiJhOOg0wFLghZSW/jogJwO7AVEl/JP0BOK/yKYOrJQ0nnXaYDUwqNQYzs95SRNPToBuVWq0WXV1d/d0NM9vISJqVrw39CX9Dy8ysAIermVkBDlczswIcrmZmBThczcwKcLiamRXgcDUzK8DhamZWgMPVzKwAh6uZWQEOVzOzAhyuZmYFOFzNzApwuJqZFeBwNTMrwOFqZlaAw9XMrACHq5lZAQ5XM7MCHK5mZgU4XM3MCnC4mpkV4HA1MyvA4WpmVoDD1cysAIermVkBDlczswIcrmZmBThczcwKcLiamRXgcDUzK6BouEoaL2mBpIWSJjdZf4ak+ZLmSrpL0pjKupckzc6P6ZXyXSQ9IOlRSddJ2qzkGMzMeqNYuEoaBFwMHAqMA46VNK6h2oNALSL2Am4Ezq+sezEi9s6PCZXyLwIXRsRY4Fng5FJjMDPrrZJHrvsCCyNiUUSsBq4FjqxWiIi7I2JlXpwBjFzbDiUJOIgUxABXAkd1tNdmZh1QMlx3AhZXlpfkslZOBm6rLG8uqUvSDEn1AN0OeC4i1qxrn5JOydt3dXd3924EZma9NLjgvtWkLJpWlI4HasABleLREbFU0q7AjyQ9BLzQ7j4jYhowDaBWqzWtY2ZWSskj1yXAqMrySGBpYyVJBwNnARMiYlW9PCKW5p+LgHuAfYCngG0k1f8oNN2nmVl/KxmuM4Gx+er+ZsAxwPRqBUn7AFNJwbqsUr6tpCH5+fbAu4H5ERHA3cDRueqJwPcKjsHMrFeKhWs+L3o6cDvwCHB9RMyTdI6k+tX/KcBQ4IaGj1ztDnRJmkMK0/MiYn5edyZwhqSFpHOw3yg1BjOz3lI6GNy41Wq16Orq6u9umNlGRtKsiKg1W9fWkaukD7ZTZmZmSbunBT7dZpmZmbGOj2JJOhQ4DNhJ0lcrq7YC1jTfyszM1vU516VAFzABmFUpXw58olSnzMw2dGsN14iYA8yRdE1E/AHSx6SAURHx7ProoJnZhqjdc653SNpK0jBgDnC5pP9XsF9mZhu0dsN164h4AfgAcHlEvA04uFy3zMw2bO3OLTBY0gjgQ6Svqm7U/u3mecxf2mwaAzPbWI1741Z8/og9Ora/do9czyF90+qxiJiZJ1N5tGO9MDPbyPgbWmZmvdSJb2iNlPQdScsk/VbSTZLWOrG1mdlA1u5pgctJM1q9kTQ59c25zMzMmmg3XIdHxOURsSY/rgCGF+yXmdkGrd1wfUrS8ZIG5cfxwNMlO2ZmtiFrN1w/RvoY1pPAb0iTVX+0VKfMzDZ07X7O9QvAifWvvOZvan2JFLpmZtag3SPXvapzCUTEM6R7WpmZWRPthusmecIW4OUj15J3jjUz26C1G5AXAPdLupF0K+sPAecW65WZ2QaurXCNiKskdQEHAQI+ULlhoJmZNWj7rX0OUweqmVkbit1a28xsIHO4mpkV4HA1MyvA4WpmVoDD1cysAIermVkBDlczswIcrmZmBThczcwKcLiamRVQNFwljZe0QNJCSZObrD9D0nxJcyXdJWlMw/qtJD0h6aJK2T15n7PzY4eSYzAz641i4SppEHAxcCgwDjhW0riGag8CtYjYC7gROL9h/ReAe5vsfmJE7J0fyzrcdTOzPit55LovsDAiFkXEauBa4MhqhYi4OyJW5sUZwMu365b0NmBH4IcF+2hmVkTJcN0JWFxZXpLLWjkZuA1A0iakOWQ/1aLu5fmUwGclqVkFSadI6pLU1d3d3fPem5n1QclwbRZ60bRiuptsDZiSi04Dbo2IxU2qT4yIPYH98+OEZvuMiGkRUYuI2vDhvgu4ma1fJW/VsgQYVVkeCSxtrCTpYOAs4ICIWJWL9wP2l3QaMBTYTNKKiJgcEU8ARMRySdeQTj9cVXAcZmY9VjJcZwJjJe0CPAEcAxxXrSBpH2AqML56YSoiJlbqnES66DVZ0mBgm4h4StKmwOHAnQXHYGbWK8XCNSLWSDoduB0YBFwWEfMknQN0RcR00mmAocAN+dTpryNiwlp2OwS4PQfrIFKwXlJqDGZmvaWIpqdBNyq1Wi26urr6uxtmtpGRNCsias3W+RtaZmYFOFzNzApwuJqZFeBwNTMrwOFqZlaAw9XMrACHq5lZAQ5XM7MCHK5mZgU4XM3MCnC4mpkV4HA1MyvA4WpmVoDD1cysAIermVkBDlczswIcrmZmBThczcwKcLiamRXgcDUzK8DhamZWgMPVzKwAh6uZWQEOVzOzAhyuZmYFOFzNzApwuJqZFeBwNTMrwOFqZlaAw9XMrICi4SppvKQFkhZKmtxk/RmS5kuaK+kuSWMa1m8l6QlJF1XK3ibpobzPr0pSyTGYmfVGsXCVNAi4GDgUGAccK2lcQ7UHgVpE7AXcCJzfsP4LwL0NZf8JnAKMzY/xHe66mVmflTxy3RdYGBGLImI1cC1wZLVCRNwdESvz4gxgZH2dpLcBOwI/rJSNALaKiJ9ERABXAUcVHIOZWa+UDNedgMWV5SW5rJWTgdsAJG0CXAB8qsk+l/Rgn2Zm/WJwwX03OxcaTStKxwM14IBcdBpwa0Qsbjil2pN9nkI6fcDo0aPb7LKZWWeUDNclwKjK8khgaWMlSQcDZwEHRMSqXLwfsL+k04ChwGaSVgBfoXLqoNU+ASJiGjANoFarNQ1gM7NSSobrTGCspF2AJ4BjgOOqFSTtA0wFxkfEsnp5REys1DmJdNFrcl5eLumdwAPAR4D/KDgGM7NeKXbONSLWAKcDtwOPANdHxDxJ50iakKtNIR2Z3iBptqTpbez6VOBSYCHwGPk8rZnZa4nSRfeNW61Wi66urv7uhpltZCTNiohas3X+hpaZWQEOVzOzAhyuZmYFOFzNzApwuJqZFeBwNTMrwOFqZlaAw9XMrACHq5lZAQ5XM7MCHK5mZgU4XM3MCnC4mpkV4HA1MyvA4WpmVoDD1cysAIermVkBDlczswIcrmZmBThczcwKcLiamRXgcDUzK8DhamZWgMPVzKwAh6uZWQEOVzOzAhyuZmYFOFzNzApwuJqZFeBwNTMrwOFqZlZA0XCVNF7SAkkLJU1usv4MSfMlzZV0l6QxuXyMpFmSZkuaJ2lSZZt78j5n58cOJcdgZtYbg0vtWNIg4GLgEGAJMFPS9IiYX6n2IFCLiJWSTgXOBz4M/AZ4V0SskjQUeDhvuzRvNzEiukr13cysr0oeue4LLIyIRRGxGrgWOLJaISLujoiVeXEGMDKXr46IVbl8SOF+mpl1XMnQ2glYXFlekstaORm4rb4gaZSkuXkfX6wctQJcnk8JfFaSmu1M0imSuiR1dXd3934UZma9UDJcm4VeNK0oHQ/UgCkvV4xYHBF7AX8GnChpx7xqYkTsCeyfHyc022dETIuIWkTUhg8f3odhmJn1XMlwXQKMqiyPBJY2VpJ0MHAWMKFyKuBl+Yh1HilIiYgn8s/lwDWk0w9mZq8pJcN1JjBW0i6SNgOOAaZXK0jaB5hKCtZllfKRkl6Xn28LvBtYIGmwpO1z+abA4cDDBcdgZtYrimj6Tr0zO5cOA74MDAIui4hzJZ0DdEXEdEl3AnuSPh0A8OuImCDpEOAC0mkEARdFxDRJWwL3AZvmfd4JnBERL62jH93A4z3s/vbAUz3cplP6s+2B3v5AHnt/t78hjn1MRDQ971g0XDdkkroiojbQ2h7o7Q/ksfd3+xvb2P0RJzOzAhyuZmYFOFxbmzZA2x7o7Q/ksfd3+xvV2H3O1cysAB+5mpkV4HA1MytgwIVrG9MgDpF0XV7/gKSdK+s+ncsXSHpfofabTsOY171UmWpxeuO2HWj7JEndlTb+rrLuREmP5seJPW27zfYvrLT9C0nPVdb1deyXSVomqemXTpR8NfdtrqS3VtZ1Yuzran9ibneupPsl/UVl3a8kPZTH3qvZ4Npo/0BJz1d+x5+rrFvr69aBtj9Vaffh/FoPy+s6MfZRku6W9IjSFKb/q0mdzr/+ETFgHqQvHjwG7ApsBswBxjXUOQ34en5+DHBdfj4u1x8C7JL3M6hA+38FbJGfn1pvPy+vKDz2k0hf2GjcdhiwKP/cNj/fttPtN9T/OOmLJ30ee97+L4G3Ag+3WH8YaeIgAe8EHujU2Nts/131/QKH1tvPy78Cti88/gOBW/r6uvWm7Ya6RwA/6vDYRwBvzc9fD/yiyb/9jr/+A+3IdZ3TIOblK/PzG4H3SlIuvzYiVkXEL4GF9Hxeg15Pw9gB7Yy9lfcBd0TEMxHxLHAHML5w+8cC3+phGy1FxH3AM2upciRwVSQzgG0kjaAzY19n+xFxf94/dPZ1b6v9tejLv5vetN3R1z23/5uI+Fl+vhx4hD+doa/jr/9AC9d2pkF8uU5ErAGeB7Zrc9tOtF/1qmkYgc2VplGcIemoQm3/bX5bdKOk+sQ763Xs+VTILsCPKsV9GXtf+teJsfdU4+sewA+V7s5xSsF295M0R9JtkvbIZett/JK2IAXXTZXijo5d6TTfPsADDas6/voXuxPBa1Q70yC2qtP2FIp9bD9VfGUaxgMqxaMjYqmkXYEfSXooIh7rYNs3A9+KdAeISaQj+IN60u8+tl93DHBjvHrOiL6MvS/968TY2++E9FekcH1Ppfjdeew7AHdI+nk+Guykn5G+J79CaU6Q7wJjWb/jPwL4cURUj3I7Nnalu5rcBPxzRLzQuLrJJn16/QfakWs70yC+XEfSYGBr0luatqZQ7ED7LadhjDxheEQsAu4h/QXuWNsR8XSlvUuAt/Wk331tv+IYGt4a9nHsfelfJ8beFkl7AZcCR0bE0/XyytiXAd+hwDSbEfFCRKzIz28FNlWagW69jZ+1v+59GrvSLHo3AVdHxLebVOn869+XE8Ub2oN0pL6I9JazfnJ+j4Y6/8irL2hdn5/vwasvaC2i5xe02ml/H9IFhLEN5dsCQ/Lz7YFH6cGFhTbbHlF5/n5gRrxyUv+XuQ/b5ufDOj32XO/NpIsY6tTYK/vZmdYXdP6GV1/Q+Gmnxt5m+6NJ5/Hf1VC+JfD6yvP7gfEF2n9D/XdOCrBf599FW69bX9rO6+sHMVt2eux5HFcBX15LnY6//j1+gTb0B+mq4C9IAXZWLjuHdJQIsDlwQ/6H/lNg18q2Z+XtFgCHFmr/TuC3wOz8mJ7L3wU8lP9xPwScXKDtfydNTD4HuBt4S2Xbj+XfyULgoyXGnpfPBs5r2K4TY/8WaWrLP5CORk4GJgGT8nqRbqj5WG6j1uGxr6v9S4FnK697Vy7fNY97Tn5tzirU/umV134GlZBv9rp1su1c5yTSBePqdp0a+3tIb+XnVn6/h5V+/f31VzOzAgbaOVczs/XC4WpmVoDD1cysAIermVkBDlczswIcrtZRku7PP3eWdFyH9/2ZZm2VIumo6uxQHd73Z9Zdq8f73FPSFZ3er/WOP4plRUg6EPhkRBzeg20GxVpuky5pRUQM7UT/2uzP/aTP4Pbpds/NxlVqLEq3q/9YRPy60/u2nvGRq3WUpBX56XnA/nkezk9IGiRpiqSZeWKYf8j1D8xzbV5D+vA2kr6bJ+qYV5+sQ9J5wOvy/q6utpXn4pyS5wJ9SNKHK/u+J09C83NJV+cZzpB0nl6ZN/dLTcaxG7CqHqySrpD0dUn/pTTX7OG5vO1xVfbdbCzHS/ppLpsqaVB9jJLOzROqzJC0Yy7/YB7vHEnV79rfTPpmofW33nzjwQ8/Wj3I867SMD8ocArwr/n5EKCL9JXKA4HfAbtU6g7LP18HPAxsV913k7b+ljQV3CBgR9JXN0fkfT9P+j74JsBPSN/WGUb6ll39nds2TcbxUeCCyvIVwA/yfsaSvmm0eU/G1azv+fnupFDcNC9/DfhIfh7AEfn5+ZW2HgJ2auw/8G7g5v7+d+BHDLhZsaz//DWwl6Sj8/LWpJBaTfoe9y8rdf9J0vvz81G53tO09h7SbF4vAb+VdC/wduCFvO8lAJJmk77jPgP4PXCppO8DtzTZ5wigu6Hs+oj4I/CopEXAW3o4rlbeS5okZ2Y+sH4dsCyvW13p3yzgkPz8x8AVkq4HqhORLAPe2EabVpjD1dYXAR+PiNtfVZjOzf6uYflgYL+IWCnpHtIR4rr23cqqyvOXgMERsUbSvqRQO4b0vfqDGrZ7kRSUVY0XKOpT0q1zXOsg4MqI+HSTdX+IfEha7z9AREyS9A7ShCOzJe0daSatzXPfrZ/5nKuVspx0S42624FT89RvSNpN0pZNttsaeDYH61tIMxTV/aG+fYP7gA/n85/DSbcV+WmrjuV5PbeONLXePwN7N6n2CPBnDWUflLSJpDeRJhVZ0INxNaqO5S7g6DxnKZKGqXLvtBZjeFNEPBARnwOe4pVp8XYjnUqxfuYjVytlLrBG0hzS+cqvkN6S/yxfVOoGmt1R4AfAJElzSeE1o7JuGjBX0s8iYmKl/DvAfqTZkwL43xHxZA7nZl4PfE/S5qSjxk80qXMfcIEkVY4cFwD3ks7rToqI30u6tM1xNXrVWCT9K2nG/U1Is0f9I/D4WrafIqk+mfVdeeyQ7sH2/Tbat8L8USyzFiR9hXRx6M78+dFbIuLGfu5WS5KGkML/PZFuUWT9yKcFzFr7v8AW/d2JHhgNTHawvjb4yNXMrAAfuZqZFeBwNTMrwOFqZlaAw9XMrACHq5lZAf8DG4IasP+XQiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "prediction = Deep_Net(train_data, train_label, layer_dims = [12288, 8, 1], learning_rate = 0.00001, num_iterations = 200)\n",
    "\n",
    "#print('\\n\\n', prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
